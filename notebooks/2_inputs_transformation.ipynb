{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a49ad190",
      "metadata": {},
      "source": [
        "# Transformaciones de variables explicativas\n",
        "\n",
        "\n",
        "**Flujo**\n",
        "1. **Batch de entrada**: Inputs (`image_frames`, `ego_history_xyz/rot`, `ego_future_xyz/rot`, etc.).\n",
        "2. **De trayectoria (xyz, rot) a acciones**: conversión de `ego_future_xyz` y `ego_future_rot` a aceleración y curvatura (Unicycle accel-curvature).\n",
        "3. **Transformaciones sobre acciones**: clipping, estandarización (z-score), discretización en bins y tokenización (DiscreteTrajectoryTokenizer).\n",
        "4. **Reconstrucción inversa**: de tokens → acciones → trayectoria (unicycle).\n",
        "5. **Ejemplo**: Batch hasta tokens y métricas de error (MAE, error en trayectoria).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87effb8d",
      "metadata": {},
      "source": [
        "## 0. Inputs\n",
        "\n",
        "Se cuentan con los siguientes campos:\n",
        "\n",
        "| Campo | Forma típica | Descripción |\n",
        "|-------|--------------|-------------|\n",
        "| `image_frames` | (N_cámaras, num_frames, 3, H, W) | Imágenes de cámaras |\n",
        "| `camera_indices` | (N_cámaras,) | Índices de cámara |\n",
        "| `ego_history_xyz` | (1, 1, num_history_steps, 3) | Posición ego en el pasado (ej. 16 steps) |\n",
        "| `ego_history_rot` | (1, 1, num_history_steps, 3, 3) | Rotación ego en el pasado |\n",
        "| **`ego_future_xyz`** | **(1, 1, num_future_steps, 3)** | **Target: posición ego futura (ej. 64 steps)** |\n",
        "| **`ego_future_rot`** | **(1, 1, num_future_steps, 3, 3)** | **Target: rotación ego futura** |\n",
        "| `relative_timestamps` | (N_cámaras, num_frames) | Timestamps relativos |\n",
        "| `absolute_timestamps` | (N_cámaras, num_frames) | Timestamps absolutos |\n",
        "| `t0_us` | escalar | Timestamp de referencia (µs) |\n",
        "| `clip_id` | str | Identificador del clip |\n",
        "\n",
        "Las **variables objetivo** que el modelo debe predecir (y sobre las que se aplican las transformaciones que justificamos aquí) se derivan de **`ego_future_xyz`** y **`ego_future_rot`**. En el config de Alpamayo, el espacio de acciones es **Unicycle accel-curvature** con 64 waypoints y `dt=0.1` s; por tanto, internamente la trayectoria futura (xyz, rot) se convierte a secuencias de **aceleración** y **curvatura**, y luego se tokenizan. Eso es lo que reconstruimos en este notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5c36d325",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['action_in_proj_cfg',\n",
              " 'action_out_proj_cfg',\n",
              " 'action_space_cfg',\n",
              " 'add_special_tokens',\n",
              " 'architectures',\n",
              " 'attn_implementation',\n",
              " 'diffusion_cfg',\n",
              " 'dtype',\n",
              " 'expert_cfg',\n",
              " 'expert_non_causal_attention',\n",
              " 'hist_traj_tokenizer_cfg',\n",
              " 'keep_same_dtype',\n",
              " 'max_pixels',\n",
              " 'min_pixels',\n",
              " 'model_dtype',\n",
              " 'model_type',\n",
              " 'tokens_per_future_traj',\n",
              " 'tokens_per_history_traj',\n",
              " 'traj_token_ids',\n",
              " 'traj_token_start_idx',\n",
              " 'traj_tokenizer_cfg',\n",
              " 'traj_vocab_size',\n",
              " 'transformers_version',\n",
              " 'vlm_backend',\n",
              " 'vocab_size']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "CFG_PATH = os.path.expanduser(\"~/Downloads/transform_config.json\")\n",
        "if not os.path.isfile(CFG_PATH):\n",
        "    CFG_PATH = \"transform_config.json\"  # \n",
        "with open(CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "list(cfg.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be3485f",
      "metadata": {},
      "source": [
        "## 1. ¿Qué aplicamos aqui?\n",
        "\n",
        "Del config tomamos principalmente:\n",
        "- `action_space_cfg` (dinámica y estadísticos)\n",
        "- `traj_tokenizer_cfg` (rango y discretización)\n",
        "- `n_waypoints`, `dt`\n",
        "- `accel_bounds`, `curvature_bounds`, `mean`, `std`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "684e94be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(64, 0.1, [-9.8, 9.8], [-0.33, 0.33], 3000)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_space = cfg[\"action_space_cfg\"]\n",
        "tokenizer = cfg[\"traj_tokenizer_cfg\"]\n",
        "\n",
        "n_waypoints = action_space[\"n_waypoints\"]\n",
        "dt = action_space[\"dt\"]\n",
        "\n",
        "accel_bounds = action_space[\"accel_bounds\"]\n",
        "curv_bounds  = action_space[\"curvature_bounds\"]\n",
        "\n",
        "accel_mean = action_space[\"accel_mean\"]\n",
        "accel_std  = action_space[\"accel_std\"]\n",
        "curv_mean  = action_space[\"curvature_mean\"]\n",
        "curv_std   = action_space[\"curvature_std\"]\n",
        "\n",
        "dims_min = tokenizer[\"dims_min\"]\n",
        "dims_max = tokenizer[\"dims_max\"]\n",
        "num_bins = tokenizer[\"num_bins\"]\n",
        "\n",
        "(n_waypoints, dt, accel_bounds, curv_bounds, num_bins)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8396897b",
      "metadata": {},
      "source": [
        "### 1.1 De ego_future_xyz y ego_future_rot a (aceleración, curvatura)\n",
        "\n",
        "El modelo no entrena directamente sobre posiciones futuras $(x,y,z)$ y matrices de rotación: usa el **espacio de acciones Unicycle** (aceleración $a_t$ y curvatura $\\kappa_t$). Hay que obtener $(a, \\kappa)$ a partir de la trayectoria.\n",
        "\n",
        "**Supuestos (defendibles en tesis):**\n",
        "- Trayectoria en marco **ENU** (East-North-Up); movimiento predominante en el plano $(x,y)$; la rotación es solo **yaw** (eje Z).\n",
        "- De `ego_future_xyz` tomamos $(x_t, y_t)$ (o solo east/north). De `ego_future_rot` extraemos el yaw $\\theta_t$ (p. ej. de la matriz 3×3 o vía atan2).\n",
        "- Velocidad escalar y curvatura:\n",
        "  - $v_t = \\| (x_{t+1}-x_t,\\, y_{t+1}-y_t) \\| / \\Delta t$\n",
        "  - $\\theta_t = \\mathrm{yaw}(\\mathrm{rot}_t)$\n",
        "  - Curvatura: $\\kappa_t = (\\theta_{t+1} - \\theta_t) / (v_t \\Delta t + \\epsilon)$ (cambio de heading por unidad de distancia).\n",
        "  - Aceleración: $a_t = (v_{t+1} - v_t) / \\Delta t$.\n",
        "\n",
        "Con esto obtenemos secuencias `accel[T]` y `curvature[T]` (longitud $T = \\texttt{num\\_future\\_steps} - 1$ o $T = 64$ si se rellenan bordes), que son las **variables objetivo** a las que luego se aplican clipping, estandarización y discretización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e04ef59e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def yaw_from_rotation_matrix(rot: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Extract yaw (rad) from a 3x3 rotation matrix (Z-axis rotation only). rot: (..., 3, 3).\"\"\"\n",
        "    # R = Rz(yaw) => cos(yaw)=R[0,0], sin(yaw)=R[1,0]\n",
        "    r = np.asarray(rot)\n",
        "    if r.ndim == 2:\n",
        "        return np.arctan2(r[1, 0], r[0, 0])\n",
        "    return np.arctan2(r[..., 1, 0], r[..., 0, 0])\n",
        "\n",
        "\n",
        "def trajectory_xyz_rot_to_accel_curvature(\n",
        "    xyz: np.ndarray,\n",
        "    rot: np.ndarray,\n",
        "    dt: float,\n",
        "    eps: float = 1e-9,\n",
        "    v_min: float = 0.5,\n",
        "    accel_bounds: tuple = (-9.8, 9.8),\n",
        "    curvature_bounds: tuple = (-0.33, 0.33),\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Convert trajectory (xyz, rot) to acceleration and curvature (Unicycle model).\n",
        "    v_min: minimum speed (m/s) for curvature computation to avoid huge kappa when v~0.\n",
        "    accel_bounds, curvature_bounds: physical clipping limits (m/s^2 and rad/m).\n",
        "    \"\"\"\n",
        "    xyz = np.squeeze(np.asarray(xyz, dtype=np.float64))\n",
        "    rot = np.squeeze(np.asarray(rot, dtype=np.float64))\n",
        "    if xyz.ndim == 3:\n",
        "        xyz = xyz.reshape(-1, 3)\n",
        "    if rot.ndim == 4:\n",
        "        rot = rot.reshape(-1, 3, 3)\n",
        "    N = xyz.shape[0]\n",
        "    assert rot.shape[0] == N\n",
        "\n",
        "    x, y = xyz[:, 0], xyz[:, 1]\n",
        "    theta = yaw_from_rotation_matrix(rot)\n",
        "\n",
        "    dx = np.diff(x)\n",
        "    dy = np.diff(y)\n",
        "    dtheta = np.diff(theta)\n",
        "\n",
        "    v_mid = np.sqrt(dx**2 + dy**2) / dt  # (N-1,)\n",
        "    v_safe = np.maximum(v_mid, v_min)  # avoid huge kappa when v~0\n",
        "\n",
        "    v0 = np.concatenate([[v_mid[0]], v_mid])\n",
        "    v1 = np.concatenate([v_mid, [v_mid[-1]]])\n",
        "    accel = (v1[1:] - v0[:-1]) / dt\n",
        "\n",
        "    # Curvature kappa = dtheta/(v*dt) in rad/m; v_safe prevents division by ~0\n",
        "    curvature = dtheta / (v_safe * dt + eps)\n",
        "\n",
        "    accel = np.clip(accel, accel_bounds[0], accel_bounds[1])\n",
        "    curvature = np.clip(curvature, curvature_bounds[0], curvature_bounds[1])\n",
        "    return accel.astype(np.float64), curvature.astype(np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ead29c0",
      "metadata": {},
      "source": [
        "## 2. Transformaciones\n",
        "\n",
        "### 2.1 Clipping por límites físicos\n",
        "Para cada paso `t`:\n",
        "\n",
        "$$\n",
        "a_t \\leftarrow \\mathrm{clip}(a_t, a_{\\min}, a_{\\max})\n",
        "$$\n",
        "$$\n",
        "\\kappa_t \\leftarrow \\mathrm{clip}(\\kappa_t, \\kappa_{\\min}, \\kappa_{\\max})\n",
        "$$\n",
        "\n",
        "Esto evita targets físicamente imposibles y estabiliza entrenamiento.\n",
        "\n",
        "### 2.2 Estandarización (z-score)\n",
        "Usando media y desviación estándar del dataset:\n",
        "\n",
        "$$\n",
        "\\hat{a}_t = \\frac{a_t - \\mu_a}{\\sigma_a}\n",
        "\\qquad\n",
        "\\hat{\\kappa}_t = \\frac{\\kappa_t - \\mu_\\kappa}{\\sigma_\\kappa}\n",
        "$$\n",
        "\n",
        "Esto pone magnitudes en escala comparable, mejora condicionamiento numérico, y ayuda a que un MLP/Transformer aprenda más fácil.\n",
        "\n",
        "### 2.3 Discretización a bins (para tokenizar)\n",
        "Si el tokenizer discretiza cada dimensión a `num_bins` en $[m, M]$, una forma estándar es:\n",
        "\n",
        "$$\n",
        "b = \\left\\lfloor \\frac{(x - m)}{(M - m)} \\cdot (B-1) \\right\\rceil\n",
        "$$\n",
        "donde:\n",
        "- $x$ es el valor continuo (p. ej. $\\hat{a}$ o $\\hat{\\kappa}$)\n",
        "- $B$ es `num_bins`\n",
        "- $\\lfloor \\cdot \\rceil$ redondeo al entero más cercano\n",
        "- luego se hace `clip(b, 0, B-1)`\n",
        "\n",
        "Y la inversión:\n",
        "\n",
        "$$\n",
        "x \\approx m + \\frac{b}{B-1}(M-m)\n",
        "$$\n",
        "\n",
        "Esto permite convertir targets continuos en **tokens discretos**, compatible con modelos tipo LLM.\n",
        "\n",
        "### 2.4 Unicycle model\n",
        "Si controlas con velocidad $v$ y curvatura $\\kappa$:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t + v_t \\kappa_t \\Delta t\n",
        "$$\n",
        "$$\n",
        "x_{t+1} = x_t + v_t \\cos(\\theta_t) \\Delta t\n",
        "$$\n",
        "$$\n",
        "y_{t+1} = y_t + v_t \\sin(\\theta_t) \\Delta t\n",
        "$$\n",
        "\n",
        "Si el control es aceleración $a$, entonces:\n",
        "$$\n",
        "v_{t+1} = v_t + a_t \\Delta t\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e0dfd3b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_bounds(x, lo, hi):\n",
        "    return np.minimum(np.maximum(x, lo), hi)\n",
        "\n",
        "def standardize(x, mean, std, eps=1e-12):\n",
        "    return (x - mean) / (std + eps)\n",
        "\n",
        "def unstandardize(z, mean, std):\n",
        "    return z * std + mean\n",
        "\n",
        "def quantize_to_bins(x, xmin, xmax, num_bins):\n",
        "    \"\"\"Map continuous x to integer bins in [0, num_bins-1].\"\"\"\n",
        "    x = clip_bounds(x, xmin, xmax)\n",
        "    # scale to [0, num_bins-1]\n",
        "    b = np.rint((x - xmin) / (xmax - xmin) * (num_bins - 1)).astype(np.int64)\n",
        "    return clip_bounds(b, 0, num_bins - 1)\n",
        "\n",
        "def dequantize_from_bins(b, xmin, xmax, num_bins):\n",
        "    b = clip_bounds(b, 0, num_bins - 1).astype(np.float64)\n",
        "    return xmin + (b / (num_bins - 1)) * (xmax - xmin)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3630d160",
      "metadata": {},
      "source": [
        "## 3. Targets continuos a tokens\n",
        "\n",
        "Aquí construimos una función que:\n",
        "1. clip: `accel` y `curvature`\n",
        "2. estandarizar con (mean, std)\n",
        "3. discretizar a bins dentro de `dims_min/dims_max`\n",
        "4. producir un par de tokens por timestep (a, kappa)\n",
        "\n",
        "`dims_min=[-10,-10]` y `dims_max=[10,10]`. Eso sugiere que la discretización opera sobre una representación ya acotada. Una interpretación razonable y defendible es que discretizas los valores **estandarizados** (z-scores) dentro de [-10, 10].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ea56273",
      "metadata": {},
      "outputs": [],
      "source": [
        "def targets_to_tokens(accel, curv, *, use_standardize=True):\n",
        "    \"\"\"accel, curv: arrays shape [T]\"\"\"\n",
        "    accel = np.asarray(accel, dtype=np.float64)\n",
        "    curv  = np.asarray(curv,  dtype=np.float64)\n",
        "    assert accel.shape == curv.shape\n",
        "\n",
        "    # 1) clip in physical bounds\n",
        "    a = clip_bounds(accel, accel_bounds[0], accel_bounds[1])\n",
        "    k = clip_bounds(curv,  curv_bounds[0],  curv_bounds[1])\n",
        "\n",
        "    # 2) standardize \n",
        "    if use_standardize:\n",
        "        a_z = standardize(a, accel_mean, accel_std)\n",
        "        k_z = standardize(k, curv_mean, curv_std)\n",
        "    else:\n",
        "        a_z, k_z = a, k\n",
        "\n",
        "    # 3) quantize to bins in dims_min/dims_max\n",
        "    a_bin = quantize_to_bins(a_z, dims_min[0], dims_max[0], num_bins)\n",
        "    k_bin = quantize_to_bins(k_z, dims_min[1], dims_max[1], num_bins)\n",
        "\n",
        "    # shape [T, 2]\n",
        "    return np.stack([a_bin, k_bin], axis=-1)\n",
        "\n",
        "def tokens_to_targets(tokens, *, use_standardize=True):\n",
        "    tokens = np.asarray(tokens, dtype=np.int64)\n",
        "    assert tokens.ndim == 2 and tokens.shape[1] == 2\n",
        "\n",
        "    a_bin = tokens[:, 0]\n",
        "    k_bin = tokens[:, 1]\n",
        "\n",
        "    # 1) dequantize back to continuous in dims range\n",
        "    a_z = dequantize_from_bins(a_bin, dims_min[0], dims_max[0], num_bins)\n",
        "    k_z = dequantize_from_bins(k_bin, dims_min[1], dims_max[1], num_bins)\n",
        "\n",
        "    # 2) unstandardize \n",
        "    if use_standardize:\n",
        "        a = unstandardize(a_z, accel_mean, accel_std)\n",
        "        k = unstandardize(k_z, curv_mean, curv_std)\n",
        "    else:\n",
        "        a, k = a_z, k_z\n",
        "\n",
        "    # 3) clip to physical bounds again for safety\n",
        "    a = clip_bounds(a, accel_bounds[0], accel_bounds[1])\n",
        "    k = clip_bounds(k, curv_bounds[0],  curv_bounds[1])\n",
        "    \n",
        "    return np.stack([a, k], axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46375bcf",
      "metadata": {},
      "source": [
        "## 4. Reconstrucción de trayectoria con modelo unicycle\n",
        "\n",
        "Integración:\n",
        "- $v_{t+1} = \\mathrm{clip}(v_t + a_t \\, dt,\\, 0,\\, v_{\\max})$ (asumimos no reversa)\n",
        "- $\\theta_{t+1} = \\theta_t + v_t \\, \\kappa_t \\, dt$\n",
        "- $(x,y)$ por Euler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19598a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: filter variable names containing 'n'\n",
        "variables_con_n = [n for n in dir() if 'n' in n]\n",
        "print(variables_con_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ffc6c43b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.004173317062556908"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def rollout_unicycle(accel, kappa, *, dt=0.1, v0=0.0, v_max=40.0, x0=0.0, y0=0.0, th0=0.0):\n",
        "    accel = np.asarray(accel, dtype=np.float64)\n",
        "    kappa = np.asarray(kappa, dtype=np.float64)\n",
        "    T = accel.shape[0]\n",
        "\n",
        "    x = np.zeros(T+1); y = np.zeros(T+1); th = np.zeros(T+1); v = np.zeros(T+1)\n",
        "    x[0], y[0], th[0], v[0] = x0, y0, th0, v0\n",
        "\n",
        "    for i in range(T):\n",
        "        v[i+1] = clip_bounds(v[i] + accel[i]*dt, 0.0, v_max)\n",
        "        th[i+1] = th[i] + v[i]*kappa[i]*dt\n",
        "        x[i+1]  = x[i] + v[i]*np.cos(th[i])*dt\n",
        "        y[i+1]  = y[i] + v[i]*np.sin(th[i])*dt\n",
        "    return x, y, th, v\n",
        "\n",
        "T = n_waypoints if 'n_waypoints' in dir() else 64\n",
        "dt_step = dt if 'dt' in dir() else 0.1\n",
        "t = np.arange(T) * dt_step\n",
        "accel = accel_mean * np.sin(2 * np.pi * 0.2 * t)\n",
        "curv = curv_mean * np.sin(2 * np.pi * 0.1 * t)\n",
        "tokens = targets_to_tokens(accel, curv, use_standardize=True)\n",
        "targets_rec = tokens_to_targets(tokens, use_standardize=True)\n",
        "accel_rec = targets_rec[:, 0]\n",
        "curv_rec = targets_rec[:, 1]\n",
        "\n",
        "# Original vs reconstructed trajectory\n",
        "x0, y0, th0, v0 = 0.0, 0.0, 0.0, 5.0\n",
        "x, y, th, v = rollout_unicycle(accel, curv, dt=dt_step, v0=v0, x0=x0, y0=y0, th0=th0)\n",
        "x_r, y_r, th_r, v_r = rollout_unicycle(accel_rec, curv_rec, dt=dt_step, v0=v0, x0=x0, y0=y0, th0=th0)\n",
        "traj_err = np.mean(np.sqrt((x-x_r)**2 + (y-y_r)**2))\n",
        "traj_err\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1f5c1d",
      "metadata": {},
      "source": [
        "### 4.1 Pipeline completo: de batch (ego_future_xyz / ego_future_rot) a tokens\n",
        "\n",
        "Aplicamos **todas** las transformaciones que reciben las variables objetivo del modelo, partiendo del formato de batch:\n",
        "\n",
        "1. **Entrada**: `ego_future_xyz` (1, 1, 64, 3), `ego_future_rot` (1, 1, 64, 3, 3).\n",
        "2. **Trayectoria a acciones**: `trajectory_xyz_rot_to_accel_curvature` - `accel` (63,), `curvature` (63,).\n",
        "3. **Acciones a tokens**: clip, estandarización, discretización - matriz de tokens (63, 2) (o 64 si se rellena; el config usa 64 waypoints).\n",
        "4. **Inversa**: tokens a acciones a trayectoria unicycle; comparamos con la original para reportar error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a65f39a",
      "metadata": {},
      "source": [
        "### 4.2 Ejemplo con un clip muestra: \n",
        "\n",
        "Cargamos un clip guardado del conjunto de train (De un dict de tensores). El archivo `.pt` suele contener las mismas keys que el batch: `image_frames`, `ego_history_xyz`, `ego_future_xyz`, `ego_future_rot`, `t0_us`, `clip_id`, etc. Aquí usamos **solo las variables objetivo** (`ego_future_xyz`, `ego_future_rot`) para aplicar el pipeline y reportar MAE con datos reales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9cec2333",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys en clip_7.pt: ['image_frames', 'camera_indices', 'ego_history_xyz', 'ego_history_rot', 'ego_future_xyz', 'ego_future_rot', 'relative_timestamps', 'absolute_timestamps', 't0_us', 'clip_id']\n",
            "  image_frames: shape (4, 4, 3, 1080, 1920), dtype torch.uint8\n",
            "  camera_indices: shape (4,), dtype torch.int64\n",
            "  ego_history_xyz: shape (1, 1, 16, 3), dtype torch.float32\n",
            "  ego_history_rot: shape (1, 1, 16, 3, 3), dtype torch.float32\n",
            "  ego_future_xyz: shape (1, 1, 64, 3), dtype torch.float32\n",
            "  ego_future_rot: shape (1, 1, 64, 3, 3), dtype torch.float32\n",
            "  relative_timestamps: shape (4, 4), dtype torch.float32\n",
            "  absolute_timestamps: shape (4, 4), dtype torch.int64\n",
            "  t0_us: int = 1760117223655282\n",
            "  clip_id: str = f91ecb1d-4039-4a81-877e-4c67c000d138\n",
            "\n",
            "t0_us: 1760117223655282   clip_id: f91ecb1d-4039-4a81-877e-4c67c000d138\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "CLIP_PT_PATH = \"clip_sample.pt\"\n",
        "if not os.path.isfile(CLIP_PT_PATH):\n",
        "    CLIP_PT_PATH = os.path.join(os.path.dirname(os.path.abspath(\".\")), \"clip_sample.pt\")\n",
        "\n",
        "clip_data = torch.load(CLIP_PT_PATH, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "print(\"Keys en clip_7.pt:\", list(clip_data.keys()))\n",
        "for k in clip_data:\n",
        "    v = clip_data[k]\n",
        "    if torch.is_tensor(v):\n",
        "        print(f\"  {k}: shape {tuple(v.shape)}, dtype {v.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {k}: {type(v).__name__} = {v}\")\n",
        "\n",
        "t0_us = clip_data.get(\"t0_us\", None)\n",
        "clip_id = clip_data.get(\"clip_id\", None)\n",
        "print(\"\\nt0_us:\", t0_us, \"  clip_id:\", clip_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bbe9241c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline clip_sample - tokens - acciones (MAE): accel = 1.1903538532351297 curv = 0.022124923116007236\n",
            "tokens_from_clip: (64, 2)  targets reconstruidos: (64, 2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ego_future_xyz_clip = clip_data[\"ego_future_xyz\"]\n",
        "ego_future_rot_clip = clip_data[\"ego_future_rot\"]\n",
        "if torch.is_tensor(ego_future_xyz_clip):\n",
        "    ego_future_xyz_clip = ego_future_xyz_clip.numpy()\n",
        "if torch.is_tensor(ego_future_rot_clip):\n",
        "    ego_future_rot_clip = ego_future_rot_clip.numpy()\n",
        "\n",
        "# 1) Trajectory to actions (accel, curvature) from clip_sample\n",
        "accel_clip, curv_clip = trajectory_xyz_rot_to_accel_curvature(\n",
        "    ego_future_xyz_clip, ego_future_rot_clip, dt=dt\n",
        ")\n",
        "# Pad to n_waypoints=64 if needed\n",
        "if len(accel_clip) == n_waypoints - 1:\n",
        "    accel_clip = np.concatenate([accel_clip, [accel_clip[-1]]])\n",
        "    curv_clip = np.concatenate([curv_clip, [curv_clip[-1]]])\n",
        "\n",
        "# 2) Actions -> tokens -> actions (targets as (64, 2))\n",
        "tokens_from_clip = targets_to_tokens(accel_clip, curv_clip, use_standardize=True)\n",
        "targets_rec_clip = tokens_to_targets(tokens_from_clip, use_standardize=True)  # (64, 2)\n",
        "\n",
        "# 3) Metrics: MAE of reconstructed actions vs original\n",
        "mae_a_clip = np.mean(np.abs(accel_clip - targets_rec_clip[:, 0]))\n",
        "mae_k_clip = np.mean(np.abs(curv_clip - targets_rec_clip[:, 1]))\n",
        "print(\"Pipeline clip_sample - tokens - acciones (MAE):\", \"accel =\", mae_a_clip, \"curv =\", mae_k_clip)\n",
        "print(\"tokens_from_clip:\", tokens_from_clip.shape, \" targets reconstruidos:\", targets_rec_clip.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c851776d",
      "metadata": {},
      "source": [
        "## 5. Conclusión\n",
        "\n",
        "- **Batch de entrada**: el modelo recibe `ego_future_xyz` y `ego_future_rot` como variables objetivo (trayectoria futura en marco ego). Se justifica la conversión a espacio de acciones (aceleración y curvatura) mediante derivación numérica de velocidad y curvatura a partir de posiciones y yaw (supuesto: movimiento en plano, rotación solo en Z).\n",
        "- **Bounds físico**: bounds de aceleración y curvatura reflejan restricciones del vehículo.\n",
        "- **Clipping**: evita targets fuera de distribución y estabiliza entrenamiento.\n",
        "- **Estandarización**: reduce escala y mejora el acondicionamiento numérico; alinea magnitudes.\n",
        "- **Discretización**: convierte control continuo en una secuencia de símbolos; esto habilita entrenamiento estilo language modeling.\n",
        "- **Error de cuantización**: medible como MAE en acciones y error en trayectoria; en el ejemplo reportamos ambos.\n",
        "- **Inversión**: demostrar que puedes volver de tokens a acciones y a trayectoria cierra el ciclo y hace la metodología reproducible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaacfe80",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
